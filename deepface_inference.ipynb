{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e695d75-3825-4b89-ab92-899049a79f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepface.detectors import SsdWrapper\n",
    "from deepface.detectors import OpenCvWrapper\n",
    "from deepface.commons import distance as dst\n",
    "from deepface.commons import functions\n",
    "from deepface import DeepFace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(detector, img):\n",
    "    detected_face = None\n",
    "    img_region = None\n",
    "    img_region = [0, 0, img.shape[1], img.shape[0]]\n",
    "\n",
    "    ssd_labels = [\"img_id\", \"is_face\", \"confidence\", \"left\", \"top\", \"right\", \"bottom\"]\n",
    "\n",
    "    target_size = (300, 300)\n",
    "\n",
    "    base_img = img.copy()  # we will restore base_img to img later\n",
    "\n",
    "    original_size = img.shape\n",
    "\n",
    "    img = cv2.resize(img, target_size)\n",
    "\n",
    "    aspect_ratio_x = original_size[1] / target_size[1]\n",
    "    aspect_ratio_y = original_size[0] / target_size[0]\n",
    "\n",
    "    imageBlob = cv2.dnn.blobFromImage(image=img)\n",
    "\n",
    "    face_detector = detector[\"face_detector\"]\n",
    "    face_detector.setInput(imageBlob)\n",
    "    detections = face_detector.forward()\n",
    "\n",
    "    detections_df = pd.DataFrame(detections[0][0], columns=ssd_labels)\n",
    "\n",
    "    detections_df = detections_df[detections_df[\"is_face\"] == 1]  # 0: background, 1: face\n",
    "    detections_df = detections_df[detections_df[\"confidence\"] >= 0.90]\n",
    "\n",
    "    detections_df[\"left\"] = (detections_df[\"left\"] * 300).astype(int)\n",
    "    detections_df[\"bottom\"] = (detections_df[\"bottom\"] * 300).astype(int)\n",
    "    detections_df[\"right\"] = (detections_df[\"right\"] * 300).astype(int)\n",
    "    detections_df[\"top\"] = (detections_df[\"top\"] * 300).astype(int)\n",
    "\n",
    "    for _, instance in detections_df.iterrows():\n",
    "\n",
    "        left = instance[\"left\"]\n",
    "        right = instance[\"right\"]\n",
    "        bottom = instance[\"bottom\"]\n",
    "        top = instance[\"top\"]\n",
    "\n",
    "        detected_face = base_img[\n",
    "            int(top * aspect_ratio_y) : int(bottom * aspect_ratio_y),\n",
    "            int(left * aspect_ratio_x) : int(right * aspect_ratio_x),\n",
    "        ]\n",
    "        img_region = [\n",
    "            int(left * aspect_ratio_x),\n",
    "            int(top * aspect_ratio_y),\n",
    "            int(right * aspect_ratio_x) - int(left * aspect_ratio_x),\n",
    "            int(bottom * aspect_ratio_y) - int(top * aspect_ratio_y),\n",
    "        ]\n",
    "        if 0 in detected_face.shape:\n",
    "            return [None, None]\n",
    "        \n",
    "        detected_face = OpenCvWrapper.align_face(detector[\"eye_detector\"], detected_face)\n",
    "        break\n",
    "\n",
    "    return [detected_face, img_region]\n",
    "\n",
    "\n",
    "def extract_faces(img, target_size):\n",
    "    current_img, current_region = detect_face(SsdWrapper.build_model(), img)\n",
    "    if current_img is not None:\n",
    "\n",
    "        # resize and padding\n",
    "        if current_img.shape[0] > 0 and current_img.shape[1] > 0:\n",
    "            factor_0 = target_size[0] / current_img.shape[0]\n",
    "            factor_1 = target_size[1] / current_img.shape[1]\n",
    "            factor = min(factor_0, factor_1)\n",
    "\n",
    "            dsize = (int(current_img.shape[1] * factor), int(current_img.shape[0] * factor))\n",
    "            current_img = cv2.resize(current_img, dsize)\n",
    "\n",
    "            diff_0 = target_size[0] - current_img.shape[0]\n",
    "            diff_1 = target_size[1] - current_img.shape[1]\n",
    "            \n",
    "            current_img = np.pad(\n",
    "                current_img,\n",
    "                (\n",
    "                    (diff_0 // 2, diff_0 - diff_0 // 2),\n",
    "                    (diff_1 // 2, diff_1 - diff_1 // 2),\n",
    "                    (0, 0),\n",
    "                ),\n",
    "                \"constant\",\n",
    "            )\n",
    "\n",
    "        # double check: if target image is not still the same size with target.\n",
    "        if current_img.shape[0:2] != target_size:\n",
    "            current_img = cv2.resize(current_img, target_size)\n",
    "\n",
    "        # normalizing the image pixels\n",
    "        current_img = image.img_to_array(current_img)  # what this line doing? must?\n",
    "        current_img = np.expand_dims(current_img, axis=0)\n",
    "        current_img /= 255  # normalize input in [0, 1]\n",
    "\n",
    "    return [current_img, current_region]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "db_path = \"data\"\n",
    "if not os.path.exists(db_path):\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facial recognition model Facenet512 is just built\n",
      "WARNING: Representations for images in data folder were previously stored in representations_facenet512.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "There are  159  representations found in  representations_facenet512.pkl\n",
      "find function lasts  1.9371917247772217  seconds\n"
     ]
    }
   ],
   "source": [
    "# paste database path like: \"C:/workspace/my_db\"\n",
    "assert len(os.listdir(db_path)) != 0, \"Database folder is empty, Please specify a database\"\n",
    "model_name=\"Facenet512\"\n",
    "detector_backend=\"ssd\"\n",
    "distance_metric=\"cosine\"\n",
    "source=0\n",
    "target_size = functions.find_target_size(model_name=model_name)\n",
    "\n",
    "# ------------------------\n",
    "model = DeepFace.build_model(model_name=model_name)\n",
    "print(f\"facial recognition model {model_name} is just built\")\n",
    "\n",
    "DeepFace.find(\n",
    "    img_path=np.zeros([400, 400, 3], dtype='uint8'),\n",
    "    db_path=db_path,\n",
    "    model_name=model_name,\n",
    "    detector_backend=detector_backend,\n",
    "    distance_metric=distance_metric,\n",
    "    enforce_detection=False,\n",
    ")\n",
    "# -----------------------\n",
    "\n",
    "file_name = f\"representations_{model_name}.pkl\"\n",
    "file_name = file_name.replace(\"-\", \"_\").lower()\n",
    "\n",
    "with open(f\"{db_path}/{file_name}\", \"rb\") as f:\n",
    "    representations = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(representations, columns=[\"identity\", f\"{model_name}_representation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(source)  # webcam\n",
    "while True:\n",
    "    tic = time.time()\n",
    "\n",
    "    _, img = cap.read()\n",
    "    if img is None:\n",
    "        break\n",
    "\n",
    "    raw_img = img.copy()\n",
    "    target_img, target_region = extract_faces(raw_img, target_size)\n",
    "\n",
    "    if target_img is not None:\n",
    "        x, y, w, h = target_region\n",
    "\n",
    "        if w > 130:  # discard small detected faces\n",
    "\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 200, 200), 5)\n",
    "\n",
    "            target_representation = model(target_img, training=False).numpy()[0].tolist()\n",
    "            result_df = df.copy()  # df will be filtered in each img\n",
    "\n",
    "            distances = []\n",
    "            for index, instance in df.iterrows():\n",
    "                source_representation = instance[f\"{model_name}_representation\"]\n",
    "\n",
    "                if distance_metric == \"cosine\":\n",
    "                    distance = dst.findCosineDistance(source_representation, target_representation)\n",
    "                elif distance_metric == \"euclidean\":\n",
    "                    distance = dst.findEuclideanDistance(source_representation, target_representation)\n",
    "                elif distance_metric == \"euclidean_l2\":\n",
    "                    distance = dst.findEuclideanDistance(\n",
    "                        dst.l2_normalize(source_representation),\n",
    "                        dst.l2_normalize(target_representation),\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"invalid distance metric passes - {distance_metric}\")\n",
    "\n",
    "                distances.append(distance)\n",
    "\n",
    "                # ---------------------------\n",
    "\n",
    "            result_df[f\"{model_name}_{distance_metric}\"] = distances\n",
    "\n",
    "            threshold = dst.findThreshold(model_name, distance_metric)\n",
    "            result_df = result_df[result_df[f\"{model_name}_{distance_metric}\"] <= threshold]\n",
    "            result_df = result_df.sort_values(\n",
    "                by=[f\"{model_name}_{distance_metric}\"], ascending=True\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "            if result_df.shape[0] > 0:\n",
    "                label = result_df.loc[0, \"identity\"].split(\"/\")[-1]\n",
    "                img = cv2.putText(img, label.split('.')[0], (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # -------------------------------\n",
    "\n",
    "    toc = time.time()\n",
    "\n",
    "    img = cv2.putText(img, str(round(1/(toc-tic), 1)), (img.shape[1]-70,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"img\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # press q to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
